{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка наличия репозиториев датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataset():\n",
    "    dataset_directory = os.path.join(CURR_DIR, 'dataset')\n",
    "    if not os.path.exists(dataset_directory):\n",
    "        os.makedirs(dataset_directory)\n",
    "\n",
    "def check_repo_dataset(class_name):\n",
    "    class_folder = os.path.join(CURR_DIR + '\\dataset', class_name)\n",
    "    if not os.path.exists(class_folder):\n",
    "        os.makedirs(class_folder)\n",
    "        return class_folder\n",
    "    else:\n",
    "        return class_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Парсер ссылки на картинку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_url(url):\n",
    "    pattern = r'img_url=([^&]+)&text='\n",
    "    match = re.search(pattern, url)\n",
    "\n",
    "    if match:\n",
    "        img_url_encoded = match.group(1)\n",
    "        img_url_decoded = img_url_encoded.replace('%2F', '/').replace('%3A', ':')\n",
    "        return img_url_decoded\n",
    "    else:\n",
    "        print('Ссылка после img_url не найдена в URL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисления необходимого количества страниц для скачивания (1 страница = 30 картинкам)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pages(num_images):\n",
    "    return num_images // 30 + (num_images % 30 > 0) if num_images > 30 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получение HTML тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_tags(mini_images):\n",
    "    if mini_images:\n",
    "        return 'img', 'serp-item__thumb', 'src'\n",
    "    else:\n",
    "        return 'a', 'serp-item__link', 'href'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закачка картинок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url, save_path):\n",
    "    try:\n",
    "        response = requests.get(url, headers={'User-Agent':'Mozilla/5.0'}, stream=True)\n",
    "        if(response.status_code == 200):\n",
    "            with open(save_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    file.write(chunk)\n",
    "            return True\n",
    "        else:\n",
    "            print(f'Не удалось загрузить изображение: {url}')\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f'Ошибка при загрузке изображения: {url}')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(query, num_images, mini_images = False):\n",
    "    pages = calc_pages(num_images)\n",
    "    class_folder = check_repo_dataset(query)\n",
    "    \n",
    "    downloaded_count = 0\n",
    "\n",
    "    base_url = 'https:'\n",
    "\n",
    "    csv_file_path = class_folder + '_dataset.csv'\n",
    "\n",
    "    #а вот это чтобы без движков было, грузим странички\n",
    "    for page in range(0, pages):\n",
    "        search_url = f'https://yandex.ru/images/search?text={query}&p={page}'\n",
    "\n",
    "        with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            csv_writer.writerow(['date', 'file_name', 'url'])\n",
    "\n",
    "        #сделал с with для автоматического закрытия соединения\n",
    "        with requests.get(search_url, headers={'User-Agent':'Mozilla/5.0'}) as response:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            tag, tag_class, tag_source = get_html_tags(mini_images)\n",
    "\n",
    "            for a in soup.find_all(tag, class_=tag_class):\n",
    "                img_url = a[tag_source]\n",
    "                # получаем полный URL изображения\n",
    "                if mini_images and not img_url.startswith('http'):\n",
    "                    img_url = base_url + img_url\n",
    "                elif img_url.startswith('/images'):\n",
    "                    img_url = parser_url(img_url)\n",
    "\n",
    "                #csv_image_filename = image_filename\n",
    "                image_filename = f'{downloaded_count:04d}.jpg'\n",
    "                image_path = os.path.join(class_folder, image_filename)\n",
    "                if(download_image(img_url, image_path)):\n",
    "                    downloaded_count += 1\n",
    "                    print(f\"Загружено изображений для {query}: {downloaded_count}/{num_images}\")\n",
    "\n",
    "                    with open(csv_file_path, 'a', newline='') as csv_file:\n",
    "                        csv_writer = csv.writer(csv_file)\n",
    "                        csv_writer.writerow([datetime.now().strftime('%Y-%m-%d'), image_filename, img_url])\n",
    "\n",
    "                if(downloaded_count >= num_images):\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    check_dataset()\n",
    "    download_images('polar bear', 5, False)\n",
    "    download_images('brown bear', 5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено изображений для brown bear: 1/5\n",
      "Загружено изображений для brown bear: 2/5\n",
      "Загружено изображений для brown bear: 3/5\n",
      "Не удалось загрузить изображение: https://vsezhivoe.ru/wp-content/uploads/2017/08/%25D0%25BC%25D0%25B5%25D0%25B4%25D0%25B2%25D0%25B5%25D0%25B4%25D1%258C-%25D0%25B8%25D0%25B4%25D1%2591%25D1%2582-%25D0%25BF%25D0%25BE-%25D0%25BB%25D0%25B5%25D1%2581%25D1%2583.jpg\n",
      "Загружено изображений для brown bear: 4/5\n",
      "Загружено изображений для brown bear: 5/5\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
