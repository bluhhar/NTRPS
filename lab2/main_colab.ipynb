{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAvRb_xH_cNt"
      },
      "source": [
        "Импорты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "V5hPmSr2-I6W"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ry2KnJ_A8v9",
        "outputId": "c37c62a7-1973-49a6-c938-b47640edbf26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FwXAlf3Bgfr"
      },
      "source": [
        "Константы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "oJ8Fzb6IBiIr"
      },
      "outputs": [],
      "source": [
        "CURR_DIR = '/content/drive/MyDrive/lab2_ntrps'\n",
        "\n",
        "#в каждом из датасетов присутствует поле date, остальные поля различаются\n",
        "IMAGES_FIELDS = ['date', 'file_name', 'url']\n",
        "NUMBER_FIELDS = ['date', ]\n",
        "TEXT_FIELDS = ['date', ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpGoQwYF_khT"
      },
      "source": [
        "Функции"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZssMDW5r_mKX"
      },
      "source": [
        "Проверка наличия репозиториев датасета"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "GCJyL5HSB4Np"
      },
      "outputs": [],
      "source": [
        "def check_repository(dir: str, name: str) -> None:\n",
        "    dataset_directory = os.path.join(dir, name)\n",
        "    if not os.path.exists(dataset_directory):\n",
        "        os.makedirs(dataset_directory)\n",
        "    return dataset_directory"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ВАЛЮТЫ"
      ],
      "metadata": {
        "id": "e3Hz585bH-Tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ids_of_currency() -> None:\n",
        "    url = 'http://www.cbr.ru/scripts/XML_daily.asp'\n",
        "    response = requests.get(url, headers={'User-Agent':'Mozilla/5.0'})\n",
        "    if(response.status_code == 200):\n",
        "        xml_page = ET.fromstring(response.content)\n",
        "\n",
        "        data = []\n",
        "        for tag in xml_page.findall('Valute'):\n",
        "            id = tag.get('ID')\n",
        "            num_code = tag.find('NumCode').text\n",
        "            char_code = tag.find('CharCode').text\n",
        "            currency_name = tag.find('Name').text\n",
        "\n",
        "            data.append([id, num_code, char_code, currency_name])\n",
        "\n",
        "        with open(CURR_DIR + f'/datasets/currency/ids_currency.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
        "            csv_writer = csv.writer(csv_file)\n",
        "            csv_writer.writerow(['id', 'num_code', 'char_code', 'currency_name'])\n",
        "            for row in data:\n",
        "                csv_writer.writerow(row)\n"
      ],
      "metadata": {
        "id": "vUpFcFcCIBlQ"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_currency_id(file_path: str, char_code: str) -> str:\n",
        "    df = pd.read_csv(file_path)\n",
        "    currency_id = df[df['char_code'] == char_code]['id'].values[0]\n",
        "    return currency_id"
      ],
      "metadata": {
        "id": "VOIUUQ8ZICMH"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_dataset(name_currency:str, start_date: str, end_date: str) -> None:\n",
        "    id_currency = get_currency_id(CURR_DIR + '/datasets/currency/ids_currency.csv', name_currency)\n",
        "    if id_currency:\n",
        "        url = f\"https://www.cbr.ru/scripts/XML_dynamic.asp?date_req1={start_date}&date_req2={end_date}&VAL_NM_RQ={id_currency}\"\n",
        "        response = requests.get(url, headers={'User-Agent':'Mozilla/5.0'})\n",
        "        #в отличии от json надо проверить всего лишь раз что страница существует, на ней и так будут все курсы валюты\n",
        "        if(response.status_code == 200):\n",
        "            xml_page = ET.fromstring(response.content)\n",
        "\n",
        "            data = []\n",
        "            for tag in xml_page.findall('Record'):\n",
        "                date = datetime.strptime(tag.get('Date'), '%d.%m.%Y').strftime('%Y-%m-%d')\n",
        "                nominal = tag.find('Nominal').text\n",
        "                value = tag.find('Value').text.replace(',', '.')\n",
        "                vunit_Rate = tag.find('VunitRate').text.replace(',', '.')\n",
        "\n",
        "                data.append([date, nominal, value, vunit_Rate])\n",
        "\n",
        "            start_date = datetime.strptime(start_date, '%d/%m/%Y').strftime('%Y%m%d')\n",
        "            end_date= datetime.strptime(end_date, '%d/%m/%Y').strftime('%Y%m%d')\n",
        "            with open(CURR_DIR + f'/datasets/currency/{name_currency}_{start_date}_{end_date}.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
        "                csv_writer = csv.writer(csv_file)\n",
        "                csv_writer.writerow(['date', 'nominal', 'value', 'vunitRate'])\n",
        "                for row in data:\n",
        "                    csv_writer.writerow(row)\n",
        "        else:\n",
        "            print('Ошибка: Дата указана неверно!')\n",
        "    else:\n",
        "        print('Ошибка: Код валюты не найден!')"
      ],
      "metadata": {
        "id": "LKUO_3-6IFc8"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "КАРТИНКИ"
      ],
      "metadata": {
        "id": "7gVfIsCqIAQI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZuxYTT3CbNP"
      },
      "source": [
        "Парсер ссылки на картинку"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "ZSLgfEpqCcF5"
      },
      "outputs": [],
      "source": [
        "def parser_url(url: str) -> str:\n",
        "    pattern = r'img_url=([^&]+)&text='\n",
        "    match = re.search(pattern, url)\n",
        "\n",
        "    if match:\n",
        "        img_url_encoded = match.group(1)\n",
        "        img_url_decoded = img_url_encoded.replace('%2F', '/').replace('%3A', ':')\n",
        "        return img_url_decoded\n",
        "    else:\n",
        "        print('Ошибка: Ссылка после img_url не найдена в URL')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFyDChj4CdLH"
      },
      "source": [
        "Вычисления необходимого количества страниц для скачивания (1 страница = 30 картинкам)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "TpD2k1kaCeYR"
      },
      "outputs": [],
      "source": [
        "def calc_pages(num_images: int) -> int:\n",
        "    return num_images // 30 + (num_images % 30 > 0) if num_images > 30 else 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMxodLbICfLU"
      },
      "source": [
        "Получение HTML тегов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "ZbJcpvz_Cgcg"
      },
      "outputs": [],
      "source": [
        "def get_html_tags(mini_images: bool) -> tuple[str, str, str]:\n",
        "    if mini_images:\n",
        "        return 'img', 'serp-item__thumb', 'src'\n",
        "    else:\n",
        "        return 'a', 'serp-item__link', 'href'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL_fsAesYq8l"
      },
      "source": [
        "Запись в CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "RTDNReIZYsKh"
      },
      "outputs": [],
      "source": [
        "def write_csv_file(path: str, data: list) -> None:\n",
        "    mode = 'w' if not os.path.exists(path) else 'a'\n",
        "    with open(path, mode, newline='') as csv_file:\n",
        "      csv_writer = csv.writer(csv_file)\n",
        "      csv_writer.writerow(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apMvV1tFChlX"
      },
      "source": [
        "Закачка картинок"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "0th9304rCiyS"
      },
      "outputs": [],
      "source": [
        "def download_image(url: str, save_path: str) -> bool:\n",
        "    try:\n",
        "        response = requests.get(url, headers={'User-Agent':'Mozilla/5.0'}, stream=True)\n",
        "        if(response.status_code == 200):\n",
        "            with open(save_path, 'wb') as file:\n",
        "                for chunk in response.iter_content(1024):\n",
        "                    file.write(chunk)\n",
        "            return True\n",
        "        else:\n",
        "            print(f'Ошибка: Не удалось загрузить изображение: {url}')\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f'Ошибка при загрузке изображения: {url}')\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "a3XH3qw7Cjyf"
      },
      "outputs": [],
      "source": [
        "def download_images(query: str, num_images: int, mini_images: bool = False) -> None:\n",
        "    pages = calc_pages(num_images)\n",
        "    class_folder = check_repository(CURR_DIR, f'datasets/images/{query}')\n",
        "\n",
        "    downloaded_count = 0\n",
        "\n",
        "    base_url = 'https:'\n",
        "\n",
        "    csv_file_path = CURR_DIR + '/datasets/images/' + f'{query}_dataset.csv'\n",
        "    write_csv_file(csv_file_path, IMAGES_FIELDS)\n",
        "    #а вот это чтобы без движков было, грузим странички\n",
        "    for page in range(0, pages):\n",
        "        search_url = f'https://yandex.ru/images/search?text={query}&p={page}'\n",
        "\n",
        "        #сделал с with для автоматического закрытия соединения\n",
        "        with requests.get(search_url, headers={'User-Agent':'Mozilla/5.0'}) as response:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            tag, tag_class, tag_source = get_html_tags(mini_images)\n",
        "\n",
        "            for a in soup.find_all(tag, class_=tag_class):\n",
        "                img_url = a[tag_source]\n",
        "                # получаем полный URL изображения\n",
        "                if mini_images and not img_url.startswith('http'):\n",
        "                    img_url = base_url + img_url\n",
        "                elif img_url.startswith('/images'):\n",
        "                    img_url = parser_url(img_url)\n",
        "\n",
        "                #csv_image_filename = image_filename\n",
        "                image_filename = f'{downloaded_count:04d}.jpg'\n",
        "                image_path = os.path.join(class_folder, image_filename)\n",
        "                if(download_image(img_url, image_path)):\n",
        "                    downloaded_count += 1\n",
        "                    print(f'Загружено изображений для {query}: {downloaded_count}/{num_images}')\n",
        "\n",
        "                    write_csv_file(csv_file_path, [datetime.now().strftime('%Y-%m-%d'), image_filename, img_url])\n",
        "\n",
        "                if(downloaded_count >= num_images):\n",
        "                    break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas()"
      ],
      "metadata": {
        "id": "6BFKf17IK05u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_dataset(df: pd.DataFrame, required_fields: list) -> bool:\n",
        "    for field in required_fields:\n",
        "        if field not in df.columns:\n",
        "            return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "FYo5WOL74Mnf"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset_from_files(files: list, fields: list) -> pd.DataFrame:\n",
        "    df = pd.DataFrame()\n",
        "    for file in files:\n",
        "        data = pd.read_csv(file)\n",
        "        if check_dataset(data, fields):\n",
        "            data['date'] = pd.to_datetime(data['date'])\n",
        "            df = df.append(data, ignore_index=True)\n",
        "        else:\n",
        "            print(f'Ошибка: Файл {file} не содержит необходимых полей')\n",
        "        #FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
        "        #df = df.append(data, ignore_index=True) --> менять на df = df.append(data)\n",
        "        #df = pd.concat(df_list, ignore_index=True) добавлять в данную строчку (просто раскомменитить)\n",
        "    return df"
      ],
      "metadata": {
        "id": "KlTsXoeGf7eI"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разделяет даты и данные"
      ],
      "metadata": {
        "id": "SVM0s9ZIOfs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def separation_date_by_data(df: pd.DataFrame) -> None:\n",
        "    df_date = df['date']\n",
        "    df_data = df.drop('date', axis=1)\n",
        "\n",
        "    df_date.to_csv(CURR_DIR + '/csv/csv_date_by_data/X.csv', index=False)\n",
        "    df_data.to_csv(CURR_DIR + '/csv/csv_date_by_data/Y.csv', index=False)"
      ],
      "metadata": {
        "id": "AZBJwZs-K2kb"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разделение по годам"
      ],
      "metadata": {
        "id": "72mn5l5cOh5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def separation_by_years(df: pd.DataFrame) -> None:\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "    for year, group in df.groupby(df['date'].dt.year):\n",
        "        start_date = group['date'].min().strftime('%Y%m%d')\n",
        "        end_date = group['date'].max().strftime('%Y%m%d')\n",
        "        filename = f'{start_date}_{end_date}.csv'\n",
        "        group.to_csv(CURR_DIR + '/csv/csv_years/' + filename, index=False)"
      ],
      "metadata": {
        "id": "Njm_SUbxOe5M"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разделение по неделям"
      ],
      "metadata": {
        "id": "DxL8J4LwUrYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def separation_by_weeks(df: pd.DataFrame) -> None:\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "    #вылазиет предупреждение вот как советует использовать данный цикл сам пандас (из-за устаревшей версии)\n",
        "    #FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
        "    #for (year, week), group in df.groupby([df['date'].dt.isocalendar().year, df['date'].dt.isocalendar().week]):\n",
        "    for (year, week), group in df.groupby([df['date'].dt.year, df['date'].dt.week]):\n",
        "        start_date = group['date'].min().strftime('%Y%m%d')\n",
        "        end_date = group['date'].max().strftime('%Y%m%d')\n",
        "        filename = f'{start_date}_{end_date}.csv'\n",
        "        group.to_csv(CURR_DIR + '/csv/csv_weeks/' + filename, index=False)"
      ],
      "metadata": {
        "id": "KrnXcu3HUszH"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 пункт"
      ],
      "metadata": {
        "id": "FRP8FulA0bvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_from_date(df: pd.DataFrame, date: datetime) -> None | pd.DataFrame:\n",
        "    data = df[df['date'] == date]\n",
        "    if data.empty:\n",
        "        return None\n",
        "    else:\n",
        "        return data.drop(columns=['date'])"
      ],
      "metadata": {
        "id": "3V7VBC310DMZ"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def next(df: pd.DataFrame, index: int) -> None | tuple[str]:\n",
        "    if index < len(df):\n",
        "        #return tuple(df.loc[index, ['date', 'file_name', 'url']]) #возвращает строки по меткам\n",
        "        return tuple(df.iloc[index]) #возвращает строки по целочисленным значениям\n",
        "    return None"
      ],
      "metadata": {
        "id": "VtuscwY2_Soe"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Итератор"
      ],
      "metadata": {
        "id": "us6l1kMadKJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataIterator:\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.df = df\n",
        "        self.counter = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.counter < len(self.df):\n",
        "            result = tuple(self.df.iloc[self.counter])\n",
        "            self.counter += 1\n",
        "            return result\n",
        "        else:\n",
        "            raise StopIteration"
      ],
      "metadata": {
        "id": "lkVRaTTackkO"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "тест функции по перезапии данных"
      ],
      "metadata": {
        "id": "SlKAX2wbcUtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rewrite_dates(df: pd.DataFrame, start_date: datetime) -> pd.DataFrame:\n",
        "    df['date'] = [start_date + pd.DateOffset(days=i) for i in range(len(df))]\n",
        "    return df"
      ],
      "metadata": {
        "id": "SavobZlDaQbd"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_new_dataset(df: pd.DataFrame, file_to_save: str, index_custom: bool = False) -> None:\n",
        "    df.to_csv(file_to_save, index=index_custom)"
      ],
      "metadata": {
        "id": "OI9leM3Ma7Rv"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ТЕСТ ДЛЯ КАРТИНОК"
      ],
      "metadata": {
        "id": "Af9JMUs3FSXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_images():\n",
        "    download_images('grey bear', 1, False)\n",
        "    #download_images('brown bear', 5, False)\n",
        "\n",
        "    #df = create_dataset_from_files([CURR_DIR + 'datasets/images/black bear_dataset.csv'], IMAGES_FIELDS)\n",
        "    #print(df)\n",
        "    #rewrite_dates(df, datetime(2023, 1, 1))\n",
        "    #print(df)\n",
        "\n",
        "    #df = create_dataset_from_files([CURR_DIR + '/csv/test_weeks_dataset.csv']) #CURR_DIR + '/csv/brown bear_dataset.csv'\n",
        "    #check_repository(CURR_DIR, 'csv/csv_date_by_data')\n",
        "    #separation_date_by_data(df)\n",
        "    #check_repository(CURR_DIR, 'csv/csv_years')\n",
        "    #separation_by_years(df)\n",
        "    #check_repository(CURR_DIR, 'csv/csv_weeks')\n",
        "    #separation_by_weeks(df)\n",
        "\n",
        "    #print('---Получение данных по определенной дате---')\n",
        "    #print(get_data_from_date(df, datetime(2023, 1, 24)))\n",
        "\n",
        "    #print('---Работа next()---')\n",
        "    #for index in range(0, len(df)):\n",
        "    #    print(next(df, index))\n",
        "\n",
        "    #print('---Работа итератора---')\n",
        "    #iterator = DataIterator(df)\n",
        "    #for item in iterator:\n",
        "    #    print(item)"
      ],
      "metadata": {
        "id": "po1mzjUtFPGz"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ТЕСТ ДЛЯ ВАЛЮТ"
      ],
      "metadata": {
        "id": "W1nd7ftaFVwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_currency():\n",
        "    get_ids_of_currency()\n",
        "    write_dataset('USD', '01/01/1991', '31/12/2023') #USD"
      ],
      "metadata": {
        "id": "HCw1SC3HHYjE"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "РЕПОЗИТОРИИ"
      ],
      "metadata": {
        "id": "NVJt7nrdJ4Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_repos():\n",
        "    check_repository(CURR_DIR, 'datasets')\n",
        "    check_repository(CURR_DIR, 'csv')\n",
        "    check_repository(CURR_DIR, 'datasets/images')\n",
        "    check_repository(CURR_DIR, 'datasets/currency')"
      ],
      "metadata": {
        "id": "pUs4nar0J3jA"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeDOnoB1ClnD"
      },
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "rjc4E0-YCmto"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    check_repos()\n",
        "    test_images()\n",
        "    #test_currency()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-Vk5Zn0Cnoc",
        "outputId": "34e1e924-90b5-4d29-eaf7-15c620c91376"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загружено изображений для grey bear: 1/1\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}